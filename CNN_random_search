import pickle
import warnings
import keras
import numpy as np
import matplotlib.pyplot as plt
from keras.wrappers.scikit_learn import KerasClassifier
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout
from keras.models import Sequential
from keras.optimizers import Adam
from keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import RandomizedSearchCV
warnings.filterwarnings('ignore')


trdata = ImageDataGenerator()
train_data = trdata.flow_from_directory(directory='Data/images/Training',
                                        target_size=(1024, 1024),
                                        color_mode='grayscale',
                                        batch_size=30,
                                        class_mode='binary')
tsdata = ImageDataGenerator()
test_data = tsdata.flow_from_directory(directory='Data/images/Validation',
                                       target_size=(1024, 1024),
                                       color_mode='grayscale',
                                       batch_size=30,
                                       class_mode='binary')

# Vgg16 CNN
def create_model(number_of_kernels_1_layer=10,
                 number_of_kernels_rest_layers=5,
                 max_pooling_val=5,
                 activation='relu',
                 dropout=0.15,
                 neurons_first_layer=300,
                 neurons_second_layer=75,
                 adam_learning_rate=0.001):
    """
    :param number_of_kernels_1_layer:
    :param number_of_kernels_rest_layers:
    :param max_pooling_val:
    :param activation:
    :param dropout:
    :param neurons_first_layer:
    :param neurons_second_layer:
    :param adam_learning_rate:
    :return:
    """
    model = Sequential()

    model.add(Conv2D(number_of_kernels_1_layer,
                     kernel_size=(3, 3),
                     activation=activation,
                     input_shape=(1024, 1024, 1)))

    model.add(Conv2D(number_of_kernels_rest_layers,
                     kernel_size=(3, 3),
                     activation=activation))

    model.add(MaxPool2D(max_pooling_val, max_pooling_val))

    model.add(Conv2D(number_of_kernels_rest_layers,
                     kernel_size=(3, 3),
                     activation=activation))

    model.add(Conv2D(number_of_kernels_rest_layers,
                     kernel_size=(3, 3),
                     activation=activation))
    model.add(MaxPool2D(max_pooling_val, max_pooling_val))

    # NN
    model.add(Flatten())

    model.add(Dense(units=neurons_first_layer, activation=activation))
    model.add(Dense(units=neurons_second_layer, activation=activation))
    model.add(Dense(units=1, activation="sigmoid"))
    model.add(Dropout(dropout))  # Regularizaci√≥n

    model.compile(optimizer=Adam(lr=adam_learning_rate),
                  loss=keras.losses.binary_crossentropy,
                  metrics=['accuracy'])
    return model



checkpoint = ModelCheckpoint("Data/best_weights.{epoch:02d}.hdf5",
                             monitor='val_loss',
                             verbose=1,
                             save_best_only=True,
                             save_weights_only=False,
                             mode='auto',
                             period=1)

early = EarlyStopping(monitor='val_loss',
                      min_delta=0,
                      patience=20,
                      verbose=1,
                      mode='auto')

model = KerasClassifier(build_fn=create_model())

# model.load_weights('Data/best_weights.11.hdf5')

number_of_kernels_1_layer = np.random.randint(1, 20)
number_of_kernels_rest_layers = np.random.randint(1, 10)
max_pooling_val = np.random.randint(1, 10)
activation = ['LeakyReLU', 'elu', 'tanh', 'relu']
dropout = (np.random.randint(0, 500)/1000.)
neurons_first_layer = np.random.randint(250, 350)
neurons_second_layer = np.random.randint(50, 150)
adam_learning_rate = (np.random.randint(0, 100)/1000.)

# Prepare the Dict for the Search
param_dist = dict(number_of_kernels_1_layer=number_of_kernels_1_layer,
                  number_of_kernels_rest_layers=number_of_kernels_rest_layers,
                  max_pooling_val=max_pooling_val,
                  activation=activation,
                  dropout=dropout,
                  neurons_first_layer=neurons_first_layer,
                  neurons_second_layer=neurons_second_layer,
                  adam_learning_rate=adam_learning_rate)

# Search in action!
n_iter_search = 10   # Number of parameter settings that are sampled.
random_search = RandomizedSearchCV(estimator=model,
                                   param_distributions=param_dist,
                                   n_iter=n_iter_search,
                                   n_jobs=-1,
                                   verbose=1)

(X_train, Y_train) = train_data.next()

random_search.fit(X_train, Y_train)

# pickle.dump(model, open('Data/CNN_model_optimized.sav', 'wb'))
# Show the results
print("Best: %f using %s" % (random_search.best_score_, random_search.best_params_))
means = random_search.cv_results_['mean_test_score']
stds = random_search.cv_results_['std_test_score']
params = random_search.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

# plt.plot(hist.history["accuracy"])
# plt.plot(hist.history['val_accuracy'])
# plt.plot(hist.history['loss'])
# plt.plot(hist.history['val_loss'])
# plt.title("Model accuracy")
# plt.ylabel("Accuracy")
# plt.xlabel("Epoch")
# plt.legend(["Accuracy", "Validation Accuracy", "Loss", "Validation Loss"])
# plt.show()

# scores = model.evaluate_generator(test_data, verbose=1)

# print('Test loss:', scores[0])
# print('Test accuracy:', scores[1])
